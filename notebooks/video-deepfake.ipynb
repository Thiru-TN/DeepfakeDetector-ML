{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12366330,"sourceType":"datasetVersion","datasetId":7790941},{"sourceId":13665092,"sourceType":"datasetVersion","datasetId":8589582}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"123e31bea58047de8a68c3938691534e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9237458ca5b7403f8d48cd22820a354b","IPY_MODEL_5596af82c7b84ee58000329aa564ccd6","IPY_MODEL_cbd039abe67b4e888be5500cf59ca4a4"],"layout":"IPY_MODEL_8cfb2f16a79e4b96ad74e3ac1304ff2b"}},"9237458ca5b7403f8d48cd22820a354b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1859868b14e42c2ae9e04f9b3b7c63b","placeholder":"​","style":"IPY_MODEL_c9294f6e7150450d81ff5cb2c7066b27","value":"model.safetensors: 100%"}},"5596af82c7b84ee58000329aa564ccd6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18f15a85266841b2a90818efa8d2f26f","max":21355344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c94516a5b2794f82b96c3e101cf54714","value":21355344}},"cbd039abe67b4e888be5500cf59ca4a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b0b14cf420648faa3386dad4a355d9a","placeholder":"​","style":"IPY_MODEL_d474a6294f914674ab68f3677e1a9f68","value":" 21.4M/21.4M [00:00&lt;00:00, 196MB/s]"}},"8cfb2f16a79e4b96ad74e3ac1304ff2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1859868b14e42c2ae9e04f9b3b7c63b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9294f6e7150450d81ff5cb2c7066b27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18f15a85266841b2a90818efa8d2f26f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94516a5b2794f82b96c3e101cf54714":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b0b14cf420648faa3386dad4a355d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d474a6294f914674ab68f3677e1a9f68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install grad-cam","metadata":{"_uuid":"35624c60-bcb1-4c0d-93bd-0cf29430a8ec","_cell_guid":"feceb5cf-a618-4293-9eda-685d6f38e9c0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-09T17:48:46.977892Z","iopub.execute_input":"2025-11-09T17:48:46.978464Z","iopub.status.idle":"2025-11-09T17:48:46.981904Z","shell.execute_reply.started":"2025-11-09T17:48:46.978442Z","shell.execute_reply":"2025-11-09T17:48:46.981170Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"'''\nfrom google.colab import files\n\nuploaded = files.upload()\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n!kaggle datasets download -d amanrawat001/celeb-df-preprocessed\n\nwith zipfile.ZipFile(\"/content/celeb-df-preprocessed.zip\", 'r') as zip_ref:\n  zip_ref.extractall()\n'''","metadata":{"_uuid":"678250d4-d3d9-4b4a-85c8-53155aab8233","_cell_guid":"330f559c-a600-4269-a5fd-55114fdbf037","trusted":true,"collapsed":false,"id":"xc_BfntSYJSN","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-09T17:48:47.000875Z","iopub.execute_input":"2025-11-09T17:48:47.001066Z","iopub.status.idle":"2025-11-09T17:48:47.020387Z","shell.execute_reply.started":"2025-11-09T17:48:47.001051Z","shell.execute_reply":"2025-11-09T17:48:47.019624Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'\\nfrom google.colab import files\\n\\nuploaded = files.upload()\\n\\n!mkdir ~/.kaggle\\n!cp kaggle.json ~/.kaggle/\\n!chmod 600 ~/.kaggle/kaggle.json\\n\\n!kaggle datasets download -d amanrawat001/celeb-df-preprocessed\\n\\nwith zipfile.ZipFile(\"/content/celeb-df-preprocessed.zip\", \\'r\\') as zip_ref:\\n  zip_ref.extractall()\\n'"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/deepfake-metadata/train_metadata.csv\")\ndf_val = pd.read_csv(\"/kaggle/input/deepfake-metadata/val_metadata.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/deepfake-metadata/test_metadata.csv\")\n\nprint(f\"Train: {len(df_train)} videos\")\nprint(f\"Val: {len(df_val)} videos\")\nprint(f\"Test: {len(df_test)} videos\")","metadata":{"_uuid":"49ad79b3-4742-4b3c-8283-db9846d3e043","_cell_guid":"7022e882-f0d4-4363-90cf-ba91ffe01f39","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-09T17:48:47.039079Z","iopub.execute_input":"2025-11-09T17:48:47.039285Z","iopub.status.idle":"2025-11-09T17:48:47.078154Z","shell.execute_reply.started":"2025-11-09T17:48:47.039264Z","shell.execute_reply":"2025-11-09T17:48:47.077634Z"}},"outputs":[{"name":"stdout","text":"Train: 2758 videos\nVal: 597 videos\nTest: 587 videos\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"df_train.head(3)","metadata":{"_uuid":"dda02789-c95c-4306-aef8-afe04521b16b","_cell_guid":"6985d5eb-d02d-42d9-9a4f-934da7ba86fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-09T17:48:47.078675Z","iopub.execute_input":"2025-11-09T17:48:47.078898Z","iopub.status.idle":"2025-11-09T17:48:47.087168Z","shell.execute_reply.started":"2025-11-09T17:48:47.078872Z","shell.execute_reply":"2025-11-09T17:48:47.086535Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"  label  type                                               path video_folder  \\\n0  real  real  /kaggle/input/faceforencispp-extracted-frames/...          437   \n1  real  real  /kaggle/input/faceforencispp-extracted-frames/...          515   \n2  real  real  /kaggle/input/faceforencispp-extracted-frames/...          248   \n\n   frames  source_id  \n0      32        437  \n1      32        515  \n2      32        248  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>type</th>\n      <th>path</th>\n      <th>video_folder</th>\n      <th>frames</th>\n      <th>source_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>real</td>\n      <td>real</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>437</td>\n      <td>32</td>\n      <td>437</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>real</td>\n      <td>real</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>515</td>\n      <td>32</td>\n      <td>515</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>real</td>\n      <td>real</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>248</td>\n      <td>32</td>\n      <td>248</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"df_val.head(3)","metadata":{"_uuid":"06b2db15-635c-43ff-b88e-dbeebda00b93","_cell_guid":"9b0aa6fa-7923-4b9a-adcb-64835835935f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-09T17:48:47.087875Z","iopub.execute_input":"2025-11-09T17:48:47.088102Z","iopub.status.idle":"2025-11-09T17:48:47.111392Z","shell.execute_reply.started":"2025-11-09T17:48:47.088087Z","shell.execute_reply":"2025-11-09T17:48:47.110680Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"  label  type                                               path video_folder  \\\n0  real  real  /kaggle/input/faceforencispp-extracted-frames/...          973   \n1  real  real  /kaggle/input/faceforencispp-extracted-frames/...          768   \n2  real  real  /kaggle/input/faceforencispp-extracted-frames/...          057   \n\n   frames  source_id  \n0      32        973  \n1      32        768  \n2      32         57  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>type</th>\n      <th>path</th>\n      <th>video_folder</th>\n      <th>frames</th>\n      <th>source_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>real</td>\n      <td>real</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>973</td>\n      <td>32</td>\n      <td>973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>real</td>\n      <td>real</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>768</td>\n      <td>32</td>\n      <td>768</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>real</td>\n      <td>real</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>057</td>\n      <td>32</td>\n      <td>57</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"df_test.tail(3)","metadata":{"_uuid":"bae62afa-f34f-44a5-9e71-fced31b875cc","_cell_guid":"05050037-3f24-4cd2-bdcd-6d98890fa4eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-09T17:48:47.112052Z","iopub.execute_input":"2025-11-09T17:48:47.112214Z","iopub.status.idle":"2025-11-09T17:48:47.133677Z","shell.execute_reply.started":"2025-11-09T17:48:47.112201Z","shell.execute_reply":"2025-11-09T17:48:47.132898Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"    label      type                                               path  \\\n584  fake  FaceSwap  /kaggle/input/faceforencispp-extracted-frames/...   \n585  fake  FaceSwap  /kaggle/input/faceforencispp-extracted-frames/...   \n586  fake  FaceSwap  /kaggle/input/faceforencispp-extracted-frames/...   \n\n    video_folder  frames  source_id  \n584      179_212      32        179  \n585      646_643      32        646  \n586      886_877      32        886  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>type</th>\n      <th>path</th>\n      <th>video_folder</th>\n      <th>frames</th>\n      <th>source_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>584</th>\n      <td>fake</td>\n      <td>FaceSwap</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>179_212</td>\n      <td>32</td>\n      <td>179</td>\n    </tr>\n    <tr>\n      <th>585</th>\n      <td>fake</td>\n      <td>FaceSwap</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>646_643</td>\n      <td>32</td>\n      <td>646</td>\n    </tr>\n    <tr>\n      <th>586</th>\n      <td>fake</td>\n      <td>FaceSwap</td>\n      <td>/kaggle/input/faceforencispp-extracted-frames/...</td>\n      <td>886_877</td>\n      <td>32</td>\n      <td>886</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"df_train.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:48:47.134594Z","iopub.execute_input":"2025-11-09T17:48:47.134878Z","iopub.status.idle":"2025-11-09T17:48:47.154488Z","shell.execute_reply.started":"2025-11-09T17:48:47.134854Z","shell.execute_reply":"2025-11-09T17:48:47.153822Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"label                                                        real\ntype                                                         real\npath            /kaggle/input/faceforencispp-extracted-frames/...\nvideo_folder                                                  437\nframes                                                         32\nsource_id                                                     437\nName: 0, dtype: object"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"df_train['path'].iloc[1000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:48:47.155212Z","iopub.execute_input":"2025-11-09T17:48:47.155441Z","iopub.status.idle":"2025-11-09T17:48:47.173590Z","shell.execute_reply.started":"2025-11-09T17:48:47.155421Z","shell.execute_reply":"2025-11-09T17:48:47.172801Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/faceforencispp-extracted-frames/fake/Face2Face/605_591'"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nimport pickle\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom PIL import Image\nimport os\nimport gc\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\n\ndf_train = pd.read_csv(\"/kaggle/input/deepfake-metadata/train_metadata.csv\")\ndf_val = pd.read_csv(\"/kaggle/input/deepfake-metadata/val_metadata.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/deepfake-metadata/test_metadata.csv\")\n\nprint(f\"Train: {len(df_train)} videos\")\nprint(f\"Val: {len(df_val)} videos\")\nprint(f\"Test: {len(df_test)} videos\")\n\nprint(\"\\nClass distribution:\")\nprint(df_train['label'].value_counts())\nprint(df_train['label'].value_counts(normalize=True))\n\nwith open('/kaggle/input/deepfake-metadata/classical_features.pkl', 'rb') as f:\n    classical_data = pickle.load(f)\n\ntrain_classical = classical_data['train_features']\nval_classical = classical_data['val_features']\ntest_classical = classical_data['test_features']\n\nprint(f\"\\nClassical features shape: Train={train_classical.shape}, Val={val_classical.shape}, Test={test_classical.shape}\")\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_classical = scaler.fit_transform(train_classical)\nval_classical = scaler.transform(val_classical)\ntest_classical = scaler.transform(test_classical)\n\nclassical_dim = train_classical.shape[1]\nprint(f\"Classical feature dimension: {classical_dim}\")\n\ntrain_transform = A.Compose([\n    A.ImageCompression(quality_lower=30, quality_upper=100, p=0.7),\n    A.OneOf([\n        A.Downscale(scale_min=0.6, scale_max=0.9, p=1),\n        A.Resize(180, 180, p=1),\n    ], p=0.5),\n    A.OneOf([\n        A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n        A.MotionBlur(blur_limit=5, p=1),\n        A.GaussianBlur(blur_limit=(3, 7), p=1.0)\n    ], p=0.3),\n    A.OneOf([\n        A.RandomBrightnessContrast(0.15, 0.15, p=1),\n        A.HueSaturationValue(10, 15, 15, p=1),\n    ], p=0.3),\n    A.HorizontalFlip(p=0.5),\n    A.Affine(shift={'x': (0, 0.05), 'y': (0, 0.05)}, scale=(0.9, 1.1), \n             rotate=(-5, 5), p=0.3),\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\nclass VideoDatasetWithClassical(Dataset):\n    def __init__(self, dataframe, classical_features, transform=None, frames_per_video=16):\n        self.df = dataframe.reset_index(drop=True)\n        self.classical_features = classical_features\n        self.transform = transform\n        self.frames_per_video = frames_per_video\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        video_path = row['path']\n        label = 1 if row['label'] == 'fake' else 0\n        \n        frame_files = sorted(os.listdir(video_path))\n        total_frames = len(frame_files)\n        \n        if total_frames == 0:\n            raise ValueError(f\"No frames found in {video_path}\")\n        \n        if total_frames <= self.frames_per_video:\n            indices = list(range(total_frames))\n        else:\n            step = total_frames / self.frames_per_video\n            indices = [int(i * step) for i in range(self.frames_per_video)]\n        \n        frames = []\n        for i in indices:\n            img_path = os.path.join(video_path, frame_files[i])\n            img = Image.open(img_path).convert('RGB')\n            img = np.array(img)\n            \n            if self.transform:\n                augmented = self.transform(image=img)\n                img = augmented['image']\n            \n            frames.append(img)\n        \n        while len(frames) < self.frames_per_video:\n            frames.append(frames[-1])\n        \n        frames_tensor = torch.stack(frames)\n        classical_feat = torch.tensor(self.classical_features[idx], dtype=torch.float32)\n        \n        return frames_tensor, classical_feat, label\n\ntrain_dataset = VideoDatasetWithClassical(df_train, train_classical, train_transform, 16)\nval_dataset = VideoDatasetWithClassical(df_val, val_classical, val_transform, 16)\ntest_dataset = VideoDatasetWithClassical(df_test, test_classical, val_transform, 16)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n\nprint(\"Dataloaders created with classical features\")\n\nclass FusionModel(nn.Module):\n    def __init__(self, model_name='efficientnet_b0', classical_dim=72, num_classes=2, \n                 hidden_size=128, num_layers=1, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n        \n        with torch.no_grad():\n            dummy = torch.randn(1, 3, 224, 224)\n            features = self.backbone(dummy)\n            self.feature_dim = features.shape[1]\n        \n        self.lstm = nn.LSTM(\n            input_size=self.feature_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0\n        )\n        \n        self.classical_proj = nn.Sequential(\n            nn.Linear(classical_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU()\n        )\n        \n        fusion_dim = hidden_size + 32\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(fusion_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_classes)\n        )\n    \n    def forward(self, video, classical_feat):\n        batch_size, num_frames, c, h, w = video.shape\n        \n        video_flat = video.view(batch_size * num_frames, c, h, w)\n        frame_features = self.backbone(video_flat)\n        frame_features = frame_features.view(batch_size, num_frames, -1)\n        \n        lstm_out, _ = self.lstm(frame_features)\n        lstm_features = lstm_out[:, -1, :]\n        \n        classical_features = self.classical_proj(classical_feat)\n        \n        fused = torch.cat([lstm_features, classical_features], dim=1)\n        \n        output = self.classifier(fused)\n        \n        return output\n\ndef train_one_epoch(model, train_loader, optimizer, loss_fn, device, scaler):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    for batch_idx, (videos, classical, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n        videos, classical, labels = videos.to(device), classical.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        with torch.amp.autocast(device_type='cuda'):\n            outputs = model(videos, classical)\n            loss = loss_fn(outputs, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        probs = torch.softmax(outputs, dim=1)[:, 1]\n        \n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_probs.extend(probs.detach().cpu().numpy())\n        \n        if batch_idx % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    avg_loss = total_loss / len(train_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"binary\")\n    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0\n    \n    return avg_loss, accuracy, f1, precision, recall, auc\n\ndef validate(model, val_loader, loss_fn, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for videos, classical, labels in tqdm(val_loader, desc=\"Validation\"):\n            videos, classical, labels = videos.to(device), classical.to(device), labels.to(device)\n            \n            with torch.amp.autocast(device_type='cuda'):\n                outputs = model(videos, classical)\n                loss = loss_fn(outputs, labels)\n            \n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            probs = torch.softmax(outputs, dim=1)[:, 1]\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    avg_loss = total_loss / len(val_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n    \n    return avg_loss, accuracy, f1, precision, recall, auc, all_probs, all_labels\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel = FusionModel('efficientnet_b0', classical_dim=classical_dim, num_classes=2, \n                    hidden_size=128, num_layers=1)\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscaler = GradScaler()\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n\ndef calculate_class_weights(df_train):\n    num_real = len(df_train[df_train['label'] == 'real'])\n    num_fake = len(df_train[df_train['label'] == 'fake'])\n    total = num_real + num_fake\n    weight_real = total / (2 * num_real)\n    weight_fake = total / (2 * num_fake)\n    return torch.tensor([weight_real, weight_fake], dtype=torch.float32)\n\nclass_weights = calculate_class_weights(df_train)\nprint(f\"\\nClass weights: Real={class_weights[0]:.4f}, Fake={class_weights[1]:.4f}\")\nloss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n\nepochs = 5\npatience = 5\nbest_val_f1 = 0\ncounter = 0\nsave_path = \"/kaggle/working/best_fusion_model.pth\"\n\nhistory = {\n    'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_auc': [],\n    'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []\n}\n\nprint(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"\\n{'='*70}\")\nprint(f\"{'TRAINING START - FUSION MODEL':^70}\")\nprint(f\"{'='*70}\\n\")\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    \n    train_loss, train_acc, train_f1, train_prec, train_rec, train_auc = train_one_epoch(\n        model, train_loader, optimizer, loss_fn, device, scaler\n    )\n    \n    val_loss, val_acc, val_f1, val_prec, val_rec, val_auc, _, _ = validate(\n        model, val_loader, loss_fn, device\n    )\n    \n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['train_f1'].append(train_f1)\n    history['train_auc'].append(train_auc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_f1'].append(val_f1)\n    history['val_auc'].append(val_auc)\n    \n    scheduler.step()\n    \n    print(f\"\\nTRAIN: Loss={train_loss:.4f} Acc={train_acc:.4f} F1={train_f1:.4f} AUC={train_auc:.4f}\")\n    print(f\"VAL:   Loss={val_loss:.4f} Acc={val_acc:.4f} F1={val_f1:.4f} AUC={val_auc:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'val_f1': val_f1,\n            'val_auc': val_auc,\n        }, save_path)\n        counter = 0\n        print(f\"Best model saved! (F1: {val_f1:.4f})\")\n    else:\n        counter += 1\n        print(f\"No improvement ({counter}/{patience})\")\n    \n    if counter >= patience:\n        print(\"Early stopping triggered\")\n        break\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T18:54:56.012857Z","iopub.execute_input":"2025-11-09T18:54:56.013140Z","iopub.status.idle":"2025-11-09T19:17:43.822208Z","shell.execute_reply.started":"2025-11-09T18:54:56.013120Z","shell.execute_reply":"2025-11-09T19:17:43.821376Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nTrain: 2758 videos\nVal: 597 videos\nTest: 587 videos\n\nClass distribution:\nlabel\nfake    2079\nreal     679\nName: count, dtype: int64\nlabel\nfake    0.753807\nreal    0.246193\nName: proportion, dtype: float64\n\nClassical features shape: Train=(2758, 72), Val=(597, 72), Test=(587, 72)\nClassical feature dimension: 72\nDataloaders created with classical features\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_38/2950970756.py:54: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n  A.ImageCompression(quality_lower=30, quality_upper=100, p=0.7),\n/tmp/ipykernel_38/2950970756.py:56: UserWarning: Argument(s) 'scale_min, scale_max' are not valid for transform Downscale\n  A.Downscale(scale_min=0.6, scale_max=0.9, p=1),\n/tmp/ipykernel_38/2950970756.py:60: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n/tmp/ipykernel_38/2950970756.py:69: UserWarning: Argument(s) 'shift' are not valid for transform Affine\n  A.Affine(shift={'x': (0, 0.05), 'y': (0, 0.05)}, scale=(0.9, 1.1),\n/tmp/ipykernel_38/2950970756.py:282: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nClass weights: Real=2.0309, Fake=0.6633\n\nModel parameters: 4,765,214\n\n======================================================================\n                    TRAINING START - FUSION MODEL                     \n======================================================================\n\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 344/344 [05:31<00:00,  1.04it/s]\nValidation: 100%|██████████| 75/75 [01:19<00:00,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.43      0.65      0.51       147\n        Fake       0.86      0.72      0.78       450\n\n    accuracy                           0.70       597\n   macro avg       0.64      0.68      0.65       597\nweighted avg       0.75      0.70      0.72       597\n\n\nTRAIN: Loss=0.6823 Acc=0.7060 F1=0.8183 AUC=0.5637\nVAL:   Loss=0.5533 Acc=0.6985 F1=0.7816 AUC=0.7685\nBest model saved! (F1: 0.7816)\n\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 344/344 [03:36<00:00,  1.59it/s]\nValidation: 100%|██████████| 75/75 [00:35<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.41      0.95      0.58       147\n        Fake       0.97      0.56      0.71       450\n\n    accuracy                           0.66       597\n   macro avg       0.69      0.76      0.65       597\nweighted avg       0.83      0.66      0.68       597\n\n\nTRAIN: Loss=0.6269 Acc=0.7525 F1=0.8361 AUC=0.7388\nVAL:   Loss=0.5681 Acc=0.6583 F1=0.7135 AUC=0.8756\nNo improvement (1/5)\n\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 344/344 [03:15<00:00,  1.76it/s]\nValidation: 100%|██████████| 75/75 [00:35<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.78      0.69      0.73       147\n        Fake       0.90      0.94      0.92       450\n\n    accuracy                           0.88       597\n   macro avg       0.84      0.81      0.83       597\nweighted avg       0.87      0.88      0.87       597\n\n\nTRAIN: Loss=0.4861 Acc=0.8252 F1=0.8827 AUC=0.8577\nVAL:   Loss=0.3363 Acc=0.8760 F1=0.9192 AUC=0.9009\nBest model saved! (F1: 0.9192)\n\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 344/344 [03:15<00:00,  1.76it/s]\nValidation: 100%|██████████| 75/75 [00:34<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.54      0.97      0.69       147\n        Fake       0.99      0.73      0.84       450\n\n    accuracy                           0.79       597\n   macro avg       0.76      0.85      0.77       597\nweighted avg       0.88      0.79      0.80       597\n\n\nTRAIN: Loss=0.4258 Acc=0.8565 F1=0.9044 AUC=0.8942\nVAL:   Loss=0.5406 Acc=0.7889 F1=0.8393 AUC=0.9277\nNo improvement (1/5)\n\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\nValidation: 100%|██████████| 75/75 [00:35<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.68      0.95      0.79       147\n        Fake       0.98      0.85      0.91       450\n\n    accuracy                           0.88       597\n   macro avg       0.83      0.90      0.85       597\nweighted avg       0.91      0.88      0.88       597\n\n\nTRAIN: Loss=0.3481 Acc=0.8964 F1=0.9305 AUC=0.9334\nVAL:   Loss=0.3785 Acc=0.8777 F1=0.9132 AUC=0.9607\nNo improvement (2/5)\n\n======================================================================\nTRAINING COMPLETE\n======================================================================\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}