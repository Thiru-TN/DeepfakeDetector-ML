{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12366330,"sourceType":"datasetVersion","datasetId":7790941},{"sourceId":13665092,"sourceType":"datasetVersion","datasetId":8589582}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport pickle\nimport joblib\nfrom scipy.stats import skew\nfrom skimage.feature import local_binary_pattern, hog\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"577eb937-fb33-46a4-a1c9-6b7d44b201fb","_cell_guid":"e4b752e2-ea1d-4a7a-87bb-3db844547880","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-11T19:51:43.695450Z","iopub.execute_input":"2025-11-11T19:51:43.695769Z","iopub.status.idle":"2025-11-11T19:51:43.702558Z","shell.execute_reply.started":"2025-11-11T19:51:43.695739Z","shell.execute_reply":"2025-11-11T19:51:43.701592Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/deepfake-metadata/train_metadata.csv\")\nval = pd.read_csv(\"/kaggle/input/deepfake-metadata/val_metadata.csv\")\ntest = pd.read_csv(\"/kaggle/input/deepfake-metadata/test_metadata.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.704169Z","iopub.execute_input":"2025-11-11T19:51:43.704508Z","iopub.status.idle":"2025-11-11T19:51:43.759474Z","shell.execute_reply.started":"2025-11-11T19:51:43.704480Z","shell.execute_reply":"2025-11-11T19:51:43.758397Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(train.shape)\nprint(val.shape)\nprint(test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.760418Z","iopub.execute_input":"2025-11-11T19:51:43.760721Z","iopub.status.idle":"2025-11-11T19:51:43.766108Z","shell.execute_reply.started":"2025-11-11T19:51:43.760682Z","shell.execute_reply":"2025-11-11T19:51:43.765256Z"}},"outputs":[{"name":"stdout","text":"(2758, 6)\n(597, 6)\n(587, 6)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.766940Z","iopub.execute_input":"2025-11-11T19:51:43.767252Z","iopub.status.idle":"2025-11-11T19:51:43.796330Z","shell.execute_reply.started":"2025-11-11T19:51:43.767227Z","shell.execute_reply":"2025-11-11T19:51:43.795261Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2758 entries, 0 to 2757\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   label         2758 non-null   object\n 1   type          2758 non-null   object\n 2   path          2758 non-null   object\n 3   video_folder  2758 non-null   object\n 4   frames        2758 non-null   int64 \n 5   source_id     2758 non-null   int64 \ndtypes: int64(2), object(4)\nmemory usage: 129.4+ KB\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train.iloc[1000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.798246Z","iopub.execute_input":"2025-11-11T19:51:43.798502Z","iopub.status.idle":"2025-11-11T19:51:43.813843Z","shell.execute_reply.started":"2025-11-11T19:51:43.798483Z","shell.execute_reply":"2025-11-11T19:51:43.812798Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"label                                                        fake\ntype                                                    Face2Face\npath            /kaggle/input/faceforencispp-extracted-frames/...\nvideo_folder                                              605_591\nframes                                                         32\nsource_id                                                     605\nName: 1000, dtype: object"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"print(train[\"path\"].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.814901Z","iopub.execute_input":"2025-11-11T19:51:43.815235Z","iopub.status.idle":"2025-11-11T19:51:43.830543Z","shell.execute_reply.started":"2025-11-11T19:51:43.815206Z","shell.execute_reply":"2025-11-11T19:51:43.829224Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/faceforencispp-extracted-frames/real/437\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class ClassicalFeatureExtractor:\n    def __init__(self):\n        self.lbp_radius = 1\n        self.lbp_points = 8\n        self.hog_orientations = 9\n        self.hog_pixels_per_cell = (8, 8)\n        self.hog_cells_per_block = (2, 2)\n        \n    def extract_lbp_features(self, image):\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        lbp = local_binary_pattern(gray, self.lbp_points, self.lbp_radius, method='uniform')\n        hist, _ = np.histogram(lbp.ravel(), bins=self.lbp_points + 2, range=(0, self.lbp_points + 2))\n        hist = hist.astype(float)\n        hist /= (hist.sum() + 1e-7)\n        return hist\n    \n    def extract_hog_features(self, image):\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        gray_small = cv2.resize(gray, (64, 64))\n        features = hog(gray_small, orientations=8,\n                      pixels_per_cell=(16, 16),\n                      cells_per_block=(1, 1),\n                      block_norm='L2-Hys', visualize=False, feature_vector=True)\n        return features\n    \n    def extract_color_stats(self, image):\n        stats = []\n        for channel in range(3):\n            ch = image[:, :, channel].flatten()\n            stats.extend([\n                np.mean(ch),\n                np.std(ch),\n                skew(ch)\n            ])\n        return np.array(stats)\n    \n    def extract_landmark_features(self, image):\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        gray_small = cv2.resize(gray, (128, 128))\n        detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n        \n        faces = detector.detectMultiScale(gray_small, 1.3, 3)\n        \n        features = np.zeros(5)\n        \n        if len(faces) > 0:\n            x, y, w, h = faces[0]\n            features[0] = w\n            features[1] = h\n            features[2] = w / (h + 1e-7)\n            features[3] = x + w/2\n            features[4] = y + h/2\n        \n        return features\n    \n    def extract_all_features(self, image_path):\n        image = cv2.imread(str(image_path))\n        if image is None:\n            return None\n        \n        image = cv2.resize(image, (128, 128))\n        \n        lbp_feats = self.extract_lbp_features(image)\n        color_feats = self.extract_color_stats(image)\n        landmark_feats = self.extract_landmark_features(image)\n        \n        all_features = np.concatenate([lbp_feats, color_feats, landmark_feats])\n        return all_features\n\ndef process_video_folder(video_path, extractor, max_frames=16):\n    video_path = Path(video_path)\n    frame_files = sorted(list(video_path.glob('*.png')) + list(video_path.glob('*.jpg')))\n    \n    if len(frame_files) == 0:\n        return None\n    \n    step = max(1, len(frame_files) // max_frames)\n    sampled_frames = frame_files[::step][:max_frames]\n    \n    features_list = []\n    for frame_file in sampled_frames:\n        feats = extractor.extract_all_features(frame_file)\n        if feats is not None:\n            features_list.append(feats)\n    \n    if len(features_list) == 0:\n        return None\n    \n    features_array = np.array(features_list)\n    aggregated = np.concatenate([\n        np.mean(features_array, axis=0),\n        np.std(features_array, axis=0),\n        np.max(features_array, axis=0)\n    ])\n    \n    return aggregated\n\ndef extract_features_for_split(metadata_df, extractor, split_name):\n    features_list = []\n    labels_list = []\n    failed_indices = []\n    \n    for idx, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=f\"Processing {split_name}\"):\n        video_features = process_video_folder(row['path'], extractor, max_frames=16)\n        \n        if video_features is not None:\n            features_list.append(video_features)\n            labels_list.append(1 if row['label'] == 'fake' else 0)\n        else:\n            failed_indices.append(idx)\n    \n    features_array = np.array(features_list)\n    labels_array = np.array(labels_list)\n    \n    print(f\"{split_name} - Extracted features shape: {features_array.shape}\")\n    print(f\"{split_name} - Labels shape: {labels_array.shape}\")\n    print(f\"{split_name} - Failed samples: {len(failed_indices)}\")\n    \n    return features_array, labels_array, failed_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.831671Z","iopub.execute_input":"2025-11-11T19:51:43.832006Z","iopub.status.idle":"2025-11-11T19:51:43.851917Z","shell.execute_reply.started":"2025-11-11T19:51:43.831975Z","shell.execute_reply":"2025-11-11T19:51:43.850818Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train = pd.read_csv(\"/kaggle/input/deepfake-metadata/train_metadata.csv\")\n    val = pd.read_csv(\"/kaggle/input/deepfake-metadata/val_metadata.csv\")\n    test = pd.read_csv(\"/kaggle/input/deepfake-metadata/test_metadata.csv\")\n    \n    extractor = ClassicalFeatureExtractor()\n    \n    print(\"Extracting train features...\")\n    train_features, train_labels, train_failed = extract_features_for_split(train, extractor, \"train\")\n    \n    print(\"\\nExtracting validation features...\")\n    val_features, val_labels, val_failed = extract_features_for_split(val, extractor, \"val\")\n    \n    print(\"\\nExtracting test features...\")\n    test_features, test_labels, test_failed = extract_features_for_split(test, extractor, \"test\")\n    \n    with open('classical_features.pkl', 'wb') as f:\n        pickle.dump({\n            'train_features': train_features,\n            'train_labels': train_labels,\n            'val_features': val_features,\n            'val_labels': val_labels,\n            'test_features': test_features,\n            'test_labels': test_labels,\n            'train_failed': train_failed,\n            'val_failed': val_failed,\n            'test_failed': test_failed\n        }, f)\n    \n    print(\"\\nFeatures saved to classical_features.pkl\")\n    print(f\"Total feature dimension: {train_features.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T19:51:43.911373Z","iopub.execute_input":"2025-11-11T19:51:43.911761Z"}},"outputs":[{"name":"stdout","text":"Extracting train features...\n","output_type":"stream"},{"name":"stderr","text":"Processing train:  27%|██▋       | 745/2758 [07:53<21:21,  1.57it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def load_features():\n    with open('/kaggle/input/deepfake-metadata/classical_features.pkl', 'rb') as f:\n        data = pickle.load(f)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_features(train_feats, val_feats, test_feats):\n    scaler = StandardScaler()\n    train_norm = scaler.fit_transform(train_feats)\n    val_norm = scaler.transform(val_feats)\n    test_norm = scaler.transform(test_feats)\n    return train_norm, val_norm, test_norm, scaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, X, y, split_name):\n    y_pred = model.predict(X)\n    y_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n    \n    acc = accuracy_score(y, y_pred)\n    prec = precision_score(y, y_pred)\n    rec = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    \n    try:\n        auc = roc_auc_score(y, y_proba)\n    except:\n        auc = 0.0\n    \n    print(f\"\\n{split_name} Results:\")\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall: {rec:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"ROC AUC: {auc:.4f}\")\n    \n    return {\n        'accuracy': acc,\n        'precision': prec,\n        'recall': rec,\n        'f1': f1,\n        'auc': auc\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_random_forest(X_train, y_train, X_val, y_val):\n    print(\"\\nTraining Random Forest...\")\n    rf = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=5, \n                                min_samples_leaf=2, random_state=42, n_jobs=-1)\n    rf.fit(X_train, y_train)\n    \n    train_metrics = evaluate_model(rf, X_train, y_train, \"Train\")\n    val_metrics = evaluate_model(rf, X_val, y_val, \"Validation\")\n    \n    return rf, train_metrics, val_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_gradient_boosting(X_train, y_train, X_val, y_val):\n    print(\"\\nTraining Gradient Boosting...\")\n    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5,\n                                   min_samples_split=5, min_samples_leaf=2, \n                                   random_state=42)\n    gb.fit(X_train, y_train)\n    \n    train_metrics = evaluate_model(gb, X_train, y_train, \"Train\")\n    val_metrics = evaluate_model(gb, X_val, y_val, \"Validation\")\n    \n    return gb, train_metrics, val_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_svm(X_train, y_train, X_val, y_val):\n    print(\"\\nTraining SVM...\")\n    svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n    svm.fit(X_train, y_train)\n    \n    train_metrics = evaluate_model(svm, X_train, y_train, \"Train\")\n    val_metrics = evaluate_model(svm, X_val, y_val, \"Validation\")\n    \n    return svm, train_metrics, val_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_logistic_regression(X_train, y_train, X_val, y_val):\n    print(\"\\nTraining Logistic Regression...\")\n    lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42, n_jobs=-1)\n    lr.fit(X_train, y_train)\n    \n    train_metrics = evaluate_model(lr, X_train, y_train, \"Train\")\n    val_metrics = evaluate_model(lr, X_val, y_val, \"Validation\")\n    \n    return lr, train_metrics, val_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    print(\"Loading features...\")\n    data = load_features()\n    \n    X_train = data['train_features']\n    y_train = data['train_labels']\n    X_val = data['val_features']\n    y_val = data['val_labels']\n    X_test = data['test_features']\n    y_test = data['test_labels']\n    \n    print(f\"Train samples: {X_train.shape[0]}, Features: {X_train.shape[1]}\")\n    print(f\"Val samples: {X_val.shape[0]}\")\n    print(f\"Test samples: {X_test.shape[0]}\")\n    \n    print(\"\\nNormalizing features...\")\n    X_train_norm, X_val_norm, X_test_norm, scaler = normalize_features(X_train, X_val, X_test)\n    \n    results = {}\n    \n    rf_model, rf_train, rf_val = train_random_forest(X_train_norm, y_train, X_val_norm, y_val)\n    results['random_forest'] = {'train': rf_train, 'val': rf_val}\n    \n    gb_model, gb_train, gb_val = train_gradient_boosting(X_train_norm, y_train, X_val_norm, y_val)\n    results['gradient_boosting'] = {'train': gb_train, 'val': gb_val}\n    \n    svm_model, svm_train, svm_val = train_svm(X_train_norm, y_train, X_val_norm, y_val)\n    results['svm'] = {'train': svm_train, 'val': svm_val}\n    \n    lr_model, lr_train, lr_val = train_logistic_regression(X_train_norm, y_train, X_val_norm, y_val)\n    results['logistic_regression'] = {'train': lr_train, 'val': lr_val}\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Model Comparison on Validation Set\")\n    print(\"=\"*50)\n    for model_name, metrics in results.items():\n        print(f\"\\n{model_name.upper()}:\")\n        print(f\"  Accuracy: {metrics['val']['accuracy']:.4f}\")\n        print(f\"  F1 Score: {metrics['val']['f1']:.4f}\")\n        print(f\"  ROC AUC: {metrics['val']['auc']:.4f}\")\n    \n    best_model_name = max(results.items(), key=lambda x: x[1]['val']['f1'])[0]\n    print(f\"\\nBest model by F1: {best_model_name}\")\n    \n    models = {\n        'random_forest': rf_model,\n        'gradient_boosting': gb_model,\n        'svm': svm_model,\n        'logistic_regression': lr_model,\n        'scaler': scaler\n    }\n    \n    joblib.dump(models, 'classical_ml_models.pkl')\n    joblib.dump(results, 'classical_ml_results.pkl')\n    print(\"\\nModels saved to classical_ml_models.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}