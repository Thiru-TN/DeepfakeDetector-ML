{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:31.592712Z",
     "iopub.status.busy": "2025-09-16T05:42:31.592540Z",
     "iopub.status.idle": "2025-09-16T05:42:36.087836Z",
     "shell.execute_reply": "2025-09-16T05:42:36.087284Z",
     "shell.execute_reply.started": "2025-09-16T05:42:31.592696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:36.089129Z",
     "iopub.status.busy": "2025-09-16T05:42:36.088762Z",
     "iopub.status.idle": "2025-09-16T05:42:36.092863Z",
     "shell.execute_reply": "2025-09-16T05:42:36.092153Z",
     "shell.execute_reply.started": "2025-09-16T05:42:36.089096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "frames_per_video = 16\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:36.094299Z",
     "iopub.status.busy": "2025-09-16T05:42:36.093809Z",
     "iopub.status.idle": "2025-09-16T05:42:36.109672Z",
     "shell.execute_reply": "2025-09-16T05:42:36.109030Z",
     "shell.execute_reply.started": "2025-09-16T05:42:36.094270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, video_paths, labels, transform=None, frames_per_video=frames_per_video):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.frames_per_video=frames_per_video\n",
    "\n",
    "      \n",
    "    def extract_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            return []\n",
    "\n",
    "        current_frame = 0\n",
    "        \n",
    "        while current_frame < self.frames_per_video:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "            current_frame += 1\n",
    "\n",
    "        if len(frames) > 0:\n",
    "            while len(frames) < (self.frames_per_video):\n",
    "                frames.append(frames[-1])\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "                \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        frames = self.extract_frames(video_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "        else:\n",
    "            frames = [torch.tensor(frame).permute(2,0,1).float() / 255.0 for frame in frames]\n",
    "\n",
    "        frames_tensor = torch.stack(frames)\n",
    "\n",
    "        return (frames_tensor, torch.tensor(label, dtype=torch.long))\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:36.111417Z",
     "iopub.status.busy": "2025-09-16T05:42:36.111189Z",
     "iopub.status.idle": "2025-09-16T05:42:36.122260Z",
     "shell.execute_reply": "2025-09-16T05:42:36.121696Z",
     "shell.execute_reply.started": "2025-09-16T05:42:36.111401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/faceforensics/FF++\"\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    \n",
    "    real_path = os.path.join(data_path, \"real\")\n",
    "    fake_path = os.path.join(data_path, \"fake\")\n",
    "\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "\n",
    "    if os.path.exists(real_path):\n",
    "        real_videos = [f for f in os.listdir(real_path)]\n",
    "\n",
    "        for video in real_videos:\n",
    "            video_paths.append(os.path.join(real_path, video))\n",
    "            labels.append(0)\n",
    "\n",
    "    if os.path.exists(fake_path):\n",
    "        fake_videos = [f for f in os.listdir(fake_path)]\n",
    "\n",
    "        for video in fake_videos:\n",
    "            video_paths.append(os.path.join(fake_path, video))\n",
    "            labels.append(1)\n",
    "\n",
    "    return video_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:36.123095Z",
     "iopub.status.busy": "2025-09-16T05:42:36.122856Z",
     "iopub.status.idle": "2025-09-16T05:42:36.203204Z",
     "shell.execute_reply": "2025-09-16T05:42:36.202705Z",
     "shell.execute_reply.started": "2025-09-16T05:42:36.123078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "[0, 1]\n",
      "['/kaggle/input/faceforensics/FF++/real/08__talking_against_wall.mp4', '/kaggle/input/faceforensics/FF++/real/14__walking_down_indoor_hall_disgust.mp4', '/kaggle/input/faceforensics/FF++/real/08__walking_down_street_outside_angry.mp4', '/kaggle/input/faceforensics/FF++/real/05__outside_talking_still_laughing.mp4', '/kaggle/input/faceforensics/FF++/real/14__exit_phone_room.mp4']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/input/faceforensics/FF++\"\n",
    "\n",
    "video_paths, labels = load_dataset(file_path)\n",
    "\n",
    "print(len(video_paths))\n",
    "print(len(labels))\n",
    "print(list(set(labels)))\n",
    "print(video_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:36.204405Z",
     "iopub.status.busy": "2025-09-16T05:42:36.204165Z",
     "iopub.status.idle": "2025-09-16T05:42:39.352670Z",
     "shell.execute_reply": "2025-09-16T05:42:39.351919Z",
     "shell.execute_reply.started": "2025-09-16T05:42:36.204382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.353823Z",
     "iopub.status.busy": "2025-09-16T05:42:39.353436Z",
     "iopub.status.idle": "2025-09-16T05:42:39.363789Z",
     "shell.execute_reply": "2025-09-16T05:42:39.363167Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.353799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(video_paths, labels, train_size=0.7, \n",
    "                                                              random_state=42, stratify=labels, shuffle=True)\n",
    "\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(temp_paths, temp_labels, train_size=0.7, \n",
    "                                                                  random_state=42, stratify=temp_labels, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.364892Z",
     "iopub.status.busy": "2025-09-16T05:42:39.364644Z",
     "iopub.status.idle": "2025-09-16T05:42:39.381467Z",
     "shell.execute_reply": "2025-09-16T05:42:39.380832Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.364875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 140, 0: 140})\n",
      "Counter({0: 42, 1: 42})\n",
      "Counter({0: 18, 1: 18})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(train_labels))\n",
    "print(Counter(val_labels))\n",
    "print(Counter(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.382212Z",
     "iopub.status.busy": "2025-09-16T05:42:39.382023Z",
     "iopub.status.idle": "2025-09-16T05:42:39.392038Z",
     "shell.execute_reply": "2025-09-16T05:42:39.391390Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.382189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(video_paths=train_paths, labels=train_labels, transform=transform, frames_per_video=frames_per_video)\n",
    "val_dataset = VideoDataset(video_paths=val_paths, labels=val_labels, transform=None, frames_per_video=frames_per_video)\n",
    "test_dataset = VideoDataset(video_paths=test_paths, labels=test_labels, transform=None, frames_per_video=frames_per_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.394592Z",
     "iopub.status.busy": "2025-09-16T05:42:39.394093Z",
     "iopub.status.idle": "2025-09-16T05:42:39.406924Z",
     "shell.execute_reply": "2025-09-16T05:42:39.406312Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.394574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.408080Z",
     "iopub.status.busy": "2025-09-16T05:42:39.407842Z",
     "iopub.status.idle": "2025-09-16T05:42:39.495005Z",
     "shell.execute_reply": "2025-09-16T05:42:39.494291Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.408057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/2676439193.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "def train_model_one_epoch(model, train_loader, optimizer, loss_fn, scheduler=None):\n",
    "\n",
    "    total_train_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "        \n",
    "    model.train()\n",
    "    train_tqdm = tqdm(train_loader, desc=\"Training: \")\n",
    "\n",
    "    for batch_idx, (frames, labels) in enumerate(train_tqdm):\n",
    "        \n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        B, T, C, H, W = frames.shape\n",
    "        frames = frames.view(B*T, C, H, W)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            output = model(frames) \n",
    "            output = output.view(B, T, -1).mean(dim=1)\n",
    "            \n",
    "            loss = loss_fn(output, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        probs = torch.softmax(output, dim=1)[:,1]\n",
    "\n",
    "        all_preds.extend(predicted.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "        all_probs.extend(probs.detach().cpu().numpy())\n",
    "        \n",
    "        train_tqdm.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    \n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    train_precision = precision_score(all_labels, all_preds, average=\"binary\")\n",
    "    train_recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
    "    train_f1 = f1_score(all_labels, all_preds, average=\"binary\")    \n",
    "    \n",
    "\n",
    "    return (\n",
    "        train_loss,\n",
    "        train_accuracy,\n",
    "        train_precision,\n",
    "        train_recall,\n",
    "        train_f1,\n",
    "        all_probs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.495958Z",
     "iopub.status.busy": "2025-09-16T05:42:39.495712Z",
     "iopub.status.idle": "2025-09-16T05:42:39.507901Z",
     "shell.execute_reply": "2025-09-16T05:42:39.507201Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.495940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def val_model_one_epoch(model, val_loader, loss_fn):\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    best_model_state = None\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "            \n",
    "        val_tqdm = tqdm(val_loader, desc=\"Validation: \")\n",
    "        \n",
    "        for frames, labels in val_tqdm:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "            B, T, C, H, W = frames.shape\n",
    "            frames = frames.view(B*T, C, H, W)\n",
    "                    \n",
    "            output = model(frames)\n",
    "            output = output.view(B, T, -1).mean(dim=1)\n",
    "            loss = loss_fn(output, labels)\n",
    "    \n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            probs = torch.softmax(output, dim=1)[:,1]\n",
    "                    \n",
    "            all_preds.extend(predicted.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            all_probs.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "            val_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    \n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_precision = precision_score(all_labels, all_preds, average=\"binary\")\n",
    "    val_recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "    val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return (\n",
    "        val_loss,\n",
    "        val_accuracy,\n",
    "        val_precision,\n",
    "        val_recall,\n",
    "        val_f1,\n",
    "        all_probs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XceptionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.508849Z",
     "iopub.status.busy": "2025-09-16T05:42:39.508648Z",
     "iopub.status.idle": "2025-09-16T05:42:39.522374Z",
     "shell.execute_reply": "2025-09-16T05:42:39.521735Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.508828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XceptionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=2, num_frames=frames_per_video):\n",
    "        super(XceptionNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 2, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.separable_conv1 = self._make_separable_conv(64, 128, 2)\n",
    "        self.separable_conv2 = self._make_separable_conv(128, 256, 2)\n",
    "        self.separable_conv3 = self._make_separable_conv(256, 512, 2)\n",
    "        \n",
    "        self.middle_blocks = nn.ModuleList([\n",
    "            self._make_separable_conv(512, 512, 1) for _ in range(8)\n",
    "        ])\n",
    "        \n",
    "        self.separable_conv4 = self._make_separable_conv(512, 1024, 2)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.temporal_conv = nn.Conv1d(1024, 512, 1)\n",
    "        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def _make_separable_conv(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            # Depthwise conv\n",
    "            nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Pointwise conv\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames = x.size(0), x.size(1)\n",
    "        \n",
    "        x = x.view(-1, x.size(2), x.size(3), x.size(4))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x, inplace=True)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x, inplace=True)\n",
    "        \n",
    "        x = self.separable_conv1(x)\n",
    "        x = self.separable_conv2(x)\n",
    "        x = self.separable_conv3(x)\n",
    "        \n",
    "        for block in self.middle_blocks:\n",
    "            x = block(x) + x  # Residual connection\n",
    "        \n",
    "        x = self.separable_conv4(x)\n",
    "        \n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = x.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # (batch, features, frames)\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.temporal_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:39.523174Z",
     "iopub.status.busy": "2025-09-16T05:42:39.523001Z",
     "iopub.status.idle": "2025-09-16T05:42:44.190993Z",
     "shell.execute_reply": "2025-09-16T05:42:44.190403Z",
     "shell.execute_reply.started": "2025-09-16T05:42:39.523160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth\" to /root/.cache/torch/hub/checkpoints/xception-43020ad28.pth\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "model = timm.create_model(\"xception\", pretrained=True, num_classes=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = model.get_classifier().in_features\n",
    "model.fc = nn.Linear(in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:44.192273Z",
     "iopub.status.busy": "2025-09-16T05:42:44.192015Z",
     "iopub.status.idle": "2025-09-16T05:42:44.399590Z",
     "shell.execute_reply": "2025-09-16T05:42:44.399010Z",
     "shell.execute_reply.started": "2025-09-16T05:42:44.192234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "#model = XceptionNet(num_classes=2, num_frames=frames_per_video).to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 0.001,\n",
    "    weight_decay = 1e-4\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0 = 8,\n",
    "    T_mult = 2,\n",
    "    eta_min = 1e-6\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:44.400481Z",
     "iopub.status.busy": "2025-09-16T05:42:44.400289Z",
     "iopub.status.idle": "2025-09-16T05:42:44.404634Z",
     "shell.execute_reply": "2025-09-16T05:42:44.403859Z",
     "shell.execute_reply.started": "2025-09-16T05:42:44.400466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_val_f1 = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 4\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:42:44.405514Z",
     "iopub.status.busy": "2025-09-16T05:42:44.405311Z",
     "iopub.status.idle": "2025-09-16T05:44:31.139638Z",
     "shell.execute_reply": "2025-09-16T05:44:31.138611Z",
     "shell.execute_reply.started": "2025-09-16T05:42:44.405499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/70 [00:00<?, ?it/s]/tmp/ipykernel_36/2676439193.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 70/70 [01:42<00:00,  1.46s/it, Loss=0.6865]\n",
      "Validation:   0%|          | 0/21 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.14 GiB is free. Process 4862 has 9.59 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 39.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/969443511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                         train_loader = train_loader, optimizer = optimizer, loss_fn = loss_fn)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     val_loss, val_accuracy, val_precision,val_recall, val_f1, val_probs = val_model_one_epoch(model = model, \n\u001b[0m\u001b[1;32m      9\u001b[0m                         val_loader = val_loader, loss_fn = loss_fn)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/2173886575.py\u001b[0m in \u001b[0;36mval_model_one_epoch\u001b[0;34m(model, val_loader, loss_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/xception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/xception.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.14 GiB is free. Process 4862 has 9.59 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 39.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f\"\\nEpoch: {epoch+1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_accuracy, train_precision, train_recall, train_f1, train_probs = train_model_one_epoch(model = model, \n",
    "                        train_loader = train_loader, optimizer = optimizer, loss_fn = loss_fn)\n",
    "\n",
    "    val_loss, val_accuracy, val_precision,val_recall, val_f1, val_probs = val_model_one_epoch(model = model, \n",
    "                        val_loader = val_loader, loss_fn = loss_fn)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\\nTRAINING METRICS: \")\n",
    "    print(f\"Loss: {train_loss}  Accuracy: {train_accuracy}  Precision: {train_precision}  Recall: {train_recall}  F1 Score: {train_f1}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\nValiation METRICS: \")\n",
    "    print(f\"Loss: {val_loss}  Accuracy: {val_accuracy}  Precision: {val_precision}  Recall: {val_recall}  F1 Score: {val_f1}\")\n",
    "\n",
    "    \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early Stopping Triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-16T05:44:31.140138Z",
     "iopub.status.idle": "2025-09-16T05:44:31.140376Z",
     "shell.execute_reply": "2025-09-16T05:44:31.140277Z",
     "shell.execute_reply.started": "2025-09-16T05:44:31.140267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4434986,
     "sourceId": 7615428,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (pytorch_vision)",
   "language": "python",
   "name": "pytorch_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
