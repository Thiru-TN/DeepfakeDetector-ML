{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7615428,"sourceType":"datasetVersion","datasetId":4434986}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2\nimport PIL\nimport os\nfrom tqdm import tqdm\nimport gc\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:31.592540Z","iopub.execute_input":"2025-09-16T05:42:31.592712Z","iopub.status.idle":"2025-09-16T05:42:36.087836Z","shell.execute_reply.started":"2025-09-16T05:42:31.592696Z","shell.execute_reply":"2025-09-16T05:42:36.087284Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"frames_per_video = 16\n\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:36.088762Z","iopub.execute_input":"2025-09-16T05:42:36.089129Z","iopub.status.idle":"2025-09-16T05:42:36.092863Z","shell.execute_reply.started":"2025-09-16T05:42:36.089096Z","shell.execute_reply":"2025-09-16T05:42:36.092153Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    \n    def __init__(self, video_paths, labels, transform=None, frames_per_video=frames_per_video):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.transform = transform\n        self.frames_per_video=frames_per_video\n\n      \n    def extract_frames(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n\n        if not cap.isOpened():\n            return []\n\n        current_frame = 0\n        \n        while current_frame < self.frames_per_video:\n            ret, frame = cap.read()\n            \n            if not ret:\n                break\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n            current_frame += 1\n\n        if len(frames) > 0:\n            while len(frames) < (self.frames_per_video):\n                frames.append(frames[-1])\n        \n        cap.release()\n        return frames\n                \n    \n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n        \n        frames = self.extract_frames(video_path)\n        \n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n\n        else:\n            frames = [torch.tensor(frame).permute(2,0,1).float() / 255.0 for frame in frames]\n\n        frames_tensor = torch.stack(frames)\n\n        return (frames_tensor, torch.tensor(label, dtype=torch.long))\n    \n\n    def __len__(self):\n        return len(self.video_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:36.093809Z","iopub.execute_input":"2025-09-16T05:42:36.094299Z","iopub.status.idle":"2025-09-16T05:42:36.109672Z","shell.execute_reply.started":"2025-09-16T05:42:36.094270Z","shell.execute_reply":"2025-09-16T05:42:36.109030Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_path = \"/kaggle/input/faceforensics/FF++\"\n\ndef load_dataset(data_path):\n    \n    real_path = os.path.join(data_path, \"real\")\n    fake_path = os.path.join(data_path, \"fake\")\n\n    video_paths = []\n    labels = []\n\n    if os.path.exists(real_path):\n        real_videos = [f for f in os.listdir(real_path)]\n\n        for video in real_videos:\n            video_paths.append(os.path.join(real_path, video))\n            labels.append(0)\n\n    if os.path.exists(fake_path):\n        fake_videos = [f for f in os.listdir(fake_path)]\n\n        for video in fake_videos:\n            video_paths.append(os.path.join(fake_path, video))\n            labels.append(1)\n\n    return video_paths, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:36.111189Z","iopub.execute_input":"2025-09-16T05:42:36.111417Z","iopub.status.idle":"2025-09-16T05:42:36.122260Z","shell.execute_reply.started":"2025-09-16T05:42:36.111401Z","shell.execute_reply":"2025-09-16T05:42:36.121696Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/input/faceforensics/FF++\"\n\nvideo_paths, labels = load_dataset(file_path)\n\nprint(len(video_paths))\nprint(len(labels))\nprint(list(set(labels)))\nprint(video_paths[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:36.122856Z","iopub.execute_input":"2025-09-16T05:42:36.123095Z","iopub.status.idle":"2025-09-16T05:42:36.203204Z","shell.execute_reply.started":"2025-09-16T05:42:36.123078Z","shell.execute_reply":"2025-09-16T05:42:36.202705Z"}},"outputs":[{"name":"stdout","text":"400\n400\n[0, 1]\n['/kaggle/input/faceforensics/FF++/real/08__talking_against_wall.mp4', '/kaggle/input/faceforensics/FF++/real/14__walking_down_indoor_hall_disgust.mp4', '/kaggle/input/faceforensics/FF++/real/08__walking_down_street_outside_angry.mp4', '/kaggle/input/faceforensics/FF++/real/05__outside_talking_still_laughing.mp4', '/kaggle/input/faceforensics/FF++/real/14__exit_phone_room.mp4']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:36.204165Z","iopub.execute_input":"2025-09-16T05:42:36.204405Z","iopub.status.idle":"2025-09-16T05:42:39.352670Z","shell.execute_reply.started":"2025-09-16T05:42:36.204382Z","shell.execute_reply":"2025-09-16T05:42:39.351919Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_paths, temp_paths, train_labels, temp_labels = train_test_split(video_paths, labels, train_size=0.7, \n                                                              random_state=42, stratify=labels, shuffle=True)\n\nval_paths, test_paths, val_labels, test_labels = train_test_split(temp_paths, temp_labels, train_size=0.7, \n                                                                  random_state=42, stratify=temp_labels, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.353436Z","iopub.execute_input":"2025-09-16T05:42:39.353823Z","iopub.status.idle":"2025-09-16T05:42:39.363789Z","shell.execute_reply.started":"2025-09-16T05:42:39.353799Z","shell.execute_reply":"2025-09-16T05:42:39.363167Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from collections import Counter\n\nprint(Counter(train_labels))\nprint(Counter(val_labels))\nprint(Counter(test_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.364644Z","iopub.execute_input":"2025-09-16T05:42:39.364892Z","iopub.status.idle":"2025-09-16T05:42:39.381467Z","shell.execute_reply.started":"2025-09-16T05:42:39.364875Z","shell.execute_reply":"2025-09-16T05:42:39.380832Z"}},"outputs":[{"name":"stdout","text":"Counter({1: 140, 0: 140})\nCounter({0: 42, 1: 42})\nCounter({0: 18, 1: 18})\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"train_dataset = VideoDataset(video_paths=train_paths, labels=train_labels, transform=transform, frames_per_video=frames_per_video)\nval_dataset = VideoDataset(video_paths=val_paths, labels=val_labels, transform=None, frames_per_video=frames_per_video)\ntest_dataset = VideoDataset(video_paths=test_paths, labels=test_labels, transform=None, frames_per_video=frames_per_video)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.382023Z","iopub.execute_input":"2025-09-16T05:42:39.382212Z","iopub.status.idle":"2025-09-16T05:42:39.392038Z","shell.execute_reply.started":"2025-09-16T05:42:39.382189Z","shell.execute_reply":"2025-09-16T05:42:39.391390Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"batch_size = 4\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.394093Z","iopub.execute_input":"2025-09-16T05:42:39.394592Z","iopub.status.idle":"2025-09-16T05:42:39.406924Z","shell.execute_reply.started":"2025-09-16T05:42:39.394574Z","shell.execute_reply":"2025-09-16T05:42:39.406312Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\n\ndef train_model_one_epoch(model, train_loader, optimizer, loss_fn, scheduler=None):\n\n    total_train_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n        \n    model.train()\n    train_tqdm = tqdm(train_loader, desc=\"Training: \")\n\n    for batch_idx, (frames, labels) in enumerate(train_tqdm):\n        \n        frames, labels = frames.to(device), labels.to(device)\n\n        B, T, C, H, W = frames.shape\n        frames = frames.view(B*T, C, H, W)\n        \n        optimizer.zero_grad()\n        \n        with autocast():\n            output = model(frames) \n            output = output.view(B, T, -1).mean(dim=1)\n            \n            loss = loss_fn(output, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(output.data, 1)\n        probs = torch.softmax(output, dim=1)[:,1]\n\n        all_preds.extend(predicted.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n        all_probs.extend(probs.detach().cpu().numpy())\n        \n        train_tqdm.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n\n    \n    train_loss = total_train_loss / len(train_loader)\n\n    train_accuracy = accuracy_score(all_labels, all_preds)\n    train_precision = precision_score(all_labels, all_preds, average=\"binary\")\n    train_recall = recall_score(all_labels, all_preds, average=\"binary\")\n    train_f1 = f1_score(all_labels, all_preds, average=\"binary\")    \n    \n\n    return (\n        train_loss,\n        train_accuracy,\n        train_precision,\n        train_recall,\n        train_f1,\n        all_probs\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.407842Z","iopub.execute_input":"2025-09-16T05:42:39.408080Z","iopub.status.idle":"2025-09-16T05:42:39.495005Z","shell.execute_reply.started":"2025-09-16T05:42:39.408057Z","shell.execute_reply":"2025-09-16T05:42:39.494291Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2676439193.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def val_model_one_epoch(model, val_loader, loss_fn):\n    \n    total_val_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    best_model_state = None\n    model.eval()\n\n    with torch.no_grad():\n            \n        val_tqdm = tqdm(val_loader, desc=\"Validation: \")\n        \n        for frames, labels in val_tqdm:\n            frames, labels = frames.to(device), labels.to(device)\n\n            B, T, C, H, W = frames.shape\n            frames = frames.view(B*T, C, H, W)\n                    \n            output = model(frames)\n            output = output.view(B, T, -1).mean(dim=1)\n            loss = loss_fn(output, labels)\n    \n            total_val_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            probs = torch.softmax(output, dim=1)[:,1]\n                    \n            all_preds.extend(predicted.detach().cpu().numpy())\n            all_labels.extend(labels.detach().cpu().numpy())\n            all_probs.extend(probs.detach().cpu().numpy())\n\n            val_tqdm.set_postfix(loss=loss.item())\n\n    \n    val_accuracy = accuracy_score(all_labels, all_preds)\n    val_precision = precision_score(all_labels, all_preds, average=\"binary\")\n    val_recall = recall_score(all_labels, all_preds, average=\"binary\")\n    val_f1 = f1_score(all_labels, all_preds, average=\"binary\")\n    val_loss = total_val_loss / len(val_loader)\n\n    print(classification_report(all_labels, all_preds))\n\n    return (\n        val_loss,\n        val_accuracy,\n        val_precision,\n        val_recall,\n        val_f1,\n        all_probs\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.495712Z","iopub.execute_input":"2025-09-16T05:42:39.495958Z","iopub.status.idle":"2025-09-16T05:42:39.507901Z","shell.execute_reply.started":"2025-09-16T05:42:39.495940Z","shell.execute_reply":"2025-09-16T05:42:39.507201Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## 1. XceptionNet","metadata":{}},{"cell_type":"code","source":"class XceptionNet(nn.Module):\n    \n    def __init__(self, num_classes=2, num_frames=frames_per_video):\n        super(XceptionNet, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 32, 3, 2, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        self.conv2 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        self.separable_conv1 = self._make_separable_conv(64, 128, 2)\n        self.separable_conv2 = self._make_separable_conv(128, 256, 2)\n        self.separable_conv3 = self._make_separable_conv(256, 512, 2)\n        \n        self.middle_blocks = nn.ModuleList([\n            self._make_separable_conv(512, 512, 1) for _ in range(8)\n        ])\n        \n        self.separable_conv4 = self._make_separable_conv(512, 1024, 2)\n        \n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        self.temporal_conv = nn.Conv1d(1024, 512, 1)\n        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n        \n    def _make_separable_conv(self, in_channels, out_channels, stride):\n        return nn.Sequential(\n            # Depthwise conv\n            nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            # Pointwise conv\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        batch_size, num_frames = x.size(0), x.size(1)\n        \n        x = x.view(-1, x.size(2), x.size(3), x.size(4))\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = nn.functional.relu(x, inplace=True)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = nn.functional.relu(x, inplace=True)\n        \n        x = self.separable_conv1(x)\n        x = self.separable_conv2(x)\n        x = self.separable_conv3(x)\n        \n        for block in self.middle_blocks:\n            x = block(x) + x  # Residual connection\n        \n        x = self.separable_conv4(x)\n        \n        x = self.global_avg_pool(x)\n        x = x.view(x.size(0), -1)\n        \n        x = x.view(batch_size, num_frames, -1)\n        \n        x = x.permute(0, 2, 1)  # (batch, features, frames)\n        x = self.temporal_conv(x)\n        x = self.temporal_pool(x)\n        x = x.squeeze(-1)\n        \n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.508648Z","iopub.execute_input":"2025-09-16T05:42:39.508849Z","iopub.status.idle":"2025-09-16T05:42:39.522374Z","shell.execute_reply.started":"2025-09-16T05:42:39.508828Z","shell.execute_reply":"2025-09-16T05:42:39.521735Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import timm\n\nmodel = timm.create_model(\"xception\", pretrained=True, num_classes=2)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nin_features = model.get_classifier().in_features\nmodel.fc = nn.Linear(in_features, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:39.523001Z","iopub.execute_input":"2025-09-16T05:42:39.523174Z","iopub.status.idle":"2025-09-16T05:42:44.190993Z","shell.execute_reply.started":"2025-09-16T05:42:39.523160Z","shell.execute_reply":"2025-09-16T05:42:44.190403Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n  model = create_fn(\nDownloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth\" to /root/.cache/torch/hub/checkpoints/xception-43020ad28.pth\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)\n#model = XceptionNet(num_classes=2, num_frames=frames_per_video).to(device)\n\noptimizer = AdamW(\n    model.parameters(),\n    lr = 0.001,\n    weight_decay = 1e-4\n)\n\nscheduler = CosineAnnealingWarmRestarts(\n    optimizer,\n    T_0 = 8,\n    T_mult = 2,\n    eta_min = 1e-6\n)\n\nloss_fn = nn.CrossEntropyLoss()\n\nepochs = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:44.192015Z","iopub.execute_input":"2025-09-16T05:42:44.192273Z","iopub.status.idle":"2025-09-16T05:42:44.399590Z","shell.execute_reply.started":"2025-09-16T05:42:44.192234Z","shell.execute_reply":"2025-09-16T05:42:44.399010Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"best_val_f1 = 0\nbest_val_loss = float(\"inf\")\npatience = 4\ncounter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:44.400289Z","iopub.execute_input":"2025-09-16T05:42:44.400481Z","iopub.status.idle":"2025-09-16T05:42:44.404634Z","shell.execute_reply.started":"2025-09-16T05:42:44.400466Z","shell.execute_reply":"2025-09-16T05:42:44.403859Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"for epoch in range(epochs):\n    \n    print(f\"\\nEpoch: {epoch+1}/{epochs}\")\n\n    train_loss, train_accuracy, train_precision, train_recall, train_f1, train_probs = train_model_one_epoch(model = model, \n                        train_loader = train_loader, optimizer = optimizer, loss_fn = loss_fn)\n\n    val_loss, val_accuracy, val_precision,val_recall, val_f1, val_probs = val_model_one_epoch(model = model, \n                        val_loader = val_loader, loss_fn = loss_fn)\n\n    scheduler.step()\n\n    print(f\"\\nTRAINING METRICS: \")\n    print(f\"Loss: {train_loss}  Accuracy: {train_accuracy}  Precision: {train_precision}  Recall: {train_recall}  F1 Score: {train_f1}\")\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    print(f\"\\nValiation METRICS: \")\n    print(f\"Loss: {val_loss}  Accuracy: {val_accuracy}  Precision: {val_precision}  Recall: {val_recall}  F1 Score: {val_f1}\")\n\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_model_state = copy.deepcopy(model.state_dict())\n\n\n    if best_val_loss > val_loss:\n        best_val_loss = val_loss\n        counter = 0\n\n    else:\n        counter += 1\n\n    if counter >= patience:\n        print(\"Early Stopping Triggered\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:42:44.405311Z","iopub.execute_input":"2025-09-16T05:42:44.405514Z","iopub.status.idle":"2025-09-16T05:44:31.139638Z","shell.execute_reply.started":"2025-09-16T05:42:44.405499Z","shell.execute_reply":"2025-09-16T05:44:31.138611Z"}},"outputs":[{"name":"stdout","text":"\nEpoch: 1/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/70 [00:00<?, ?it/s]/tmp/ipykernel_36/2676439193.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 70/70 [01:42<00:00,  1.46s/it, Loss=0.6865]\nValidation:   0%|          | 0/21 [00:04<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/969443511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                         train_loader = train_loader, optimizer = optimizer, loss_fn = loss_fn)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     val_loss, val_accuracy, val_precision,val_recall, val_f1, val_probs = val_model_one_epoch(model = model, \n\u001b[0m\u001b[1;32m      9\u001b[0m                         val_loader = val_loader, loss_fn = loss_fn)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2173886575.py\u001b[0m in \u001b[0;36mval_model_one_epoch\u001b[0;34m(model, val_loader, loss_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/xception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/xception.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.14 GiB is free. Process 4862 has 9.59 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 39.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.14 GiB is free. Process 4862 has 9.59 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 39.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"if best_model_state:\n    model.load_state_dict(best_model_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:44:31.140138Z","iopub.status.idle":"2025-09-16T05:44:31.140376Z","shell.execute_reply.started":"2025-09-16T05:44:31.140267Z","shell.execute_reply":"2025-09-16T05:44:31.140277Z"}},"outputs":[],"execution_count":null}]}
