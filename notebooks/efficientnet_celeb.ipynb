{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2atgI8zoYJSM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, confusion_matrix\n",
        "\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "y2KTDSfNYJSN",
        "outputId": "3a892843-c1fa-41f0-99b1-378270d8ab4d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee53f9d5-6af7-429b-9505-c7049899bafb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ee53f9d5-6af7-429b-9505-c7049899bafb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xc_BfntSYJSN"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kjRY4dYYJSO",
        "outputId": "c6a80781-504a-45ae-fa09-dc2996ee5070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/amanrawat001/celeb-df-preprocessed\n",
            "License(s): CC0-1.0\n",
            "Downloading celeb-df-preprocessed.zip to /content\n",
            " 98% 294M/300M [00:00<00:00, 857MB/s] \n",
            "100% 300M/300M [00:00<00:00, 898MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d amanrawat001/celeb-df-preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Dk8BzsgYJSO"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/celeb-df-preprocessed.zip\", 'r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h8GsvlDEYJSO"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data_path = \"/content/Celeb-DF Preprocessed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GMXQoU5iYJSO",
        "outputId": "b35ddbde-a3a4-43fc-c186-543a2a5c2ad5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f6i7sVH9YJSP"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g2--21CBYJSQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {image_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Sduft_EcYJSQ",
        "outputId": "9f3ea2b9-7d09-4b65-937c-f160f93a1966"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass ImageDataset(Dataset):\\n\\n    def __init__(self, image_paths, labels, transform=None, cache=True):\\n        self.image_paths = image_paths\\n        self.labels = labels\\n        self.transform = transform\\n        self.cache = cache\\n        self.cached_images = [None] * len(image_paths)\\n\\n\\n    def __getitem__(self, idx):\\n        if self.cache and self.cached_images[idx] is not None:\\n            image = self.cached_images[idx]\\n        else:\\n            image_path = self.image_paths[idx]\\n            label = self.labels[idx]\\n\\n            try:\\n                image = Image.open(image_path).convert(\"RGB\")\\n            except Exception as e:\\n                print(f\"Error loading {image_path}: {e}\")\\n                image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\\n\\n            if self.transform:\\n                image = self.transform(image)\\n            else:\\n                image = transforms.ToTensor()(image)\\n\\n            if self.cache:\\n                self.cached_images[idx] = image\\n\\n        return image, torch.tensor(self.labels[idx], dtype=torch.long)\\n\\n\\n    def __len__(self):\\n        return len(self.image_paths)\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, labels, transform=None, cache=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.cache = cache\n",
        "        self.cached_images = [None] * len(image_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.cache and self.cached_images[idx] is not None:\n",
        "            image = self.cached_images[idx]\n",
        "        else:\n",
        "            image_path = self.image_paths[idx]\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            try:\n",
        "                image = Image.open(image_path).convert(\"RGB\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {image_path}: {e}\")\n",
        "                image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            else:\n",
        "                image = transforms.ToTensor()(image)\n",
        "\n",
        "            if self.cache:\n",
        "                self.cached_images[idx] = image\n",
        "\n",
        "        return image, torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "        '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R1dyrowwYJSR"
      },
      "outputs": [],
      "source": [
        "def load_image_dataset(base_path):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_path = os.path.join(base_path, split)\n",
        "        if not os.path.exists(split_path):\n",
        "            continue\n",
        "\n",
        "        for label_type in ['real', 'fake']:\n",
        "            label_path = os.path.join(split_path, label_type)\n",
        "            if not os.path.exists(label_path):\n",
        "                continue\n",
        "\n",
        "            label_value = 0 if label_type == 'real' else 1\n",
        "\n",
        "            for img_file in os.listdir(label_path):\n",
        "                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_paths.append(os.path.join(label_path, img_file))\n",
        "                    labels.append(label_value)\n",
        "\n",
        "    return image_paths, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WG9zcnvFYJSR"
      },
      "outputs": [],
      "source": [
        "def load_split_dataset(base_path, split):\n",
        "    \"\"\"Load specific split (train/val/test)\"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    split_path = os.path.join(base_path, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        return [], []\n",
        "\n",
        "    for label_type in ['real', 'fake']:\n",
        "        label_path = os.path.join(split_path, label_type)\n",
        "        if not os.path.exists(label_path):\n",
        "            continue\n",
        "\n",
        "        label_value = 0 if label_type == 'real' else 1\n",
        "\n",
        "        for img_file in os.listdir(label_path):\n",
        "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_paths.append(os.path.join(label_path, img_file))\n",
        "                labels.append(label_value)\n",
        "\n",
        "    return image_paths, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Bv0Zaqu5YJSR"
      },
      "source": [
        "# Model - EfficientNet b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-mygghjRYJSR"
      },
      "outputs": [],
      "source": [
        "class EfficientNetClassifier(nn.Module):\n",
        "    def __init__(self, model_name='efficientnet_b3', num_classes=2, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy_input)\n",
        "            feature_dim = features.shape[1]\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.classifier(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UptzbxeKYJSR"
      },
      "source": [
        "# Train and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ET-T2JjEYJSR"
      },
      "outputs": [],
      "source": [
        "def train_model_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    train_tqdm = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for images, labels in train_tqdm:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "        all_preds.extend(predicted.detach().cpu().numpy())\n",
        "        all_labels.extend(labels.detach().cpu().numpy())\n",
        "        all_probs.extend(probs.detach().cpu().numpy())\n",
        "\n",
        "        train_tqdm.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
        "    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0\n",
        "\n",
        "    return avg_loss, accuracy, f1, precision, recall, auc, all_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B--5SaPXYJSS"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, val_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_tqdm = tqdm(val_loader, desc=\"Validation\")\n",
        "\n",
        "        for images, labels in val_tqdm:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "            all_preds.extend(predicted.detach().cpu().numpy())\n",
        "            all_labels.extend(labels.detach().cpu().numpy())\n",
        "            all_probs.extend(probs.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "    return avg_loss, accuracy, f1, precision, recall, auc, all_probs, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "123e31bea58047de8a68c3938691534e",
            "9237458ca5b7403f8d48cd22820a354b",
            "5596af82c7b84ee58000329aa564ccd6",
            "cbd039abe67b4e888be5500cf59ca4a4",
            "8cfb2f16a79e4b96ad74e3ac1304ff2b",
            "b1859868b14e42c2ae9e04f9b3b7c63b",
            "c9294f6e7150450d81ff5cb2c7066b27",
            "18f15a85266841b2a90818efa8d2f26f",
            "c94516a5b2794f82b96c3e101cf54714",
            "6b0b14cf420648faa3386dad4a355d9a",
            "d474a6294f914674ab68f3677e1a9f68"
          ]
        },
        "id": "WmdM3rlWYJSS",
        "outputId": "8029f22f-b856-46c8-c03f-9a94591d6f8a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Train samples: 45766\n",
            "Val samples: 13075\n",
            "Test samples: 6540\n",
            "Train label distribution: Counter({1: 34865, 0: 10901})\n",
            "Val label distribution: Counter({1: 9961, 0: 3114})\n",
            "Test label distribution: Counter({1: 4982, 0: 1558})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "123e31bea58047de8a68c3938691534e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training: EfficientNetClassifier\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:59<00:00,  3.98it/s, Loss=0.2245]\n",
            "Validation: 100%|██████████| 409/409 [00:33<00:00, 12.09it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.98      0.73      0.83      3114\n",
            "        Fake       0.92      0.99      0.96      9961\n",
            "\n",
            "    accuracy                           0.93     13075\n",
            "   macro avg       0.95      0.86      0.89     13075\n",
            "weighted avg       0.93      0.93      0.93     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.2738 | Acc: 0.8878 | F1: 0.9293\n",
            "  Prec: 0.8931 | Rec: 0.9686 | AUC: 0.9241\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1694 | Acc: 0.9309 | F1: 0.9564\n",
            "  Prec: 0.9210 | Rec: 0.9946 | AUC: 0.9702\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:55<00:00,  4.03it/s, Loss=0.0740]\n",
            "Validation: 100%|██████████| 409/409 [00:33<00:00, 12.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.96      0.82      0.88      3114\n",
            "        Fake       0.95      0.99      0.97      9961\n",
            "\n",
            "    accuracy                           0.95     13075\n",
            "   macro avg       0.95      0.90      0.92     13075\n",
            "weighted avg       0.95      0.95      0.95     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.1736 | Acc: 0.9320 | F1: 0.9565\n",
            "  Prec: 0.9341 | Rec: 0.9800 | AUC: 0.9700\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1390 | Acc: 0.9482 | F1: 0.9668\n",
            "  Prec: 0.9463 | Rec: 0.9881 | AUC: 0.9785\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:57<00:00,  4.00it/s, Loss=1.4716]\n",
            "Validation: 100%|██████████| 409/409 [00:34<00:00, 11.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.94      0.85      0.89      3114\n",
            "        Fake       0.95      0.98      0.97      9961\n",
            "\n",
            "    accuracy                           0.95     13075\n",
            "   macro avg       0.95      0.92      0.93     13075\n",
            "weighted avg       0.95      0.95      0.95     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.1445 | Acc: 0.9450 | F1: 0.9646\n",
            "  Prec: 0.9466 | Rec: 0.9833 | AUC: 0.9791\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1282 | Acc: 0.9513 | F1: 0.9685\n",
            "  Prec: 0.9535 | Rec: 0.9840 | AUC: 0.9824\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:56<00:00,  4.01it/s, Loss=0.0176]\n",
            "Validation: 100%|██████████| 409/409 [00:34<00:00, 11.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.84      0.90      3114\n",
            "        Fake       0.95      0.99      0.97      9961\n",
            "\n",
            "    accuracy                           0.95     13075\n",
            "   macro avg       0.96      0.91      0.93     13075\n",
            "weighted avg       0.96      0.95      0.95     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.1247 | Acc: 0.9533 | F1: 0.9698\n",
            "  Prec: 0.9536 | Rec: 0.9867 | AUC: 0.9837\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1249 | Acc: 0.9549 | F1: 0.9710\n",
            "  Prec: 0.9511 | Rec: 0.9918 | AUC: 0.9830\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:53<00:00,  4.05it/s, Loss=0.0129]\n",
            "Validation: 100%|██████████| 409/409 [00:33<00:00, 12.09it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.86      0.91      3114\n",
            "        Fake       0.96      0.99      0.97      9961\n",
            "\n",
            "    accuracy                           0.96     13075\n",
            "   macro avg       0.96      0.92      0.94     13075\n",
            "weighted avg       0.96      0.96      0.96     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.1081 | Acc: 0.9606 | F1: 0.9746\n",
            "  Prec: 0.9595 | Rec: 0.9901 | AUC: 0.9873\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1188 | Acc: 0.9600 | F1: 0.9742\n",
            "  Prec: 0.9568 | Rec: 0.9923 | AUC: 0.9851\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:52<00:00,  4.06it/s, Loss=0.0318]\n",
            "Validation: 100%|██████████| 409/409 [00:30<00:00, 13.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.94      0.88      0.91      3114\n",
            "        Fake       0.96      0.98      0.97      9961\n",
            "\n",
            "    accuracy                           0.96     13075\n",
            "   macro avg       0.95      0.93      0.94     13075\n",
            "weighted avg       0.96      0.96      0.96     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0983 | Acc: 0.9645 | F1: 0.9770\n",
            "  Prec: 0.9630 | Rec: 0.9915 | AUC: 0.9896\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1230 | Acc: 0.9579 | F1: 0.9727\n",
            "  Prec: 0.9631 | Rec: 0.9824 | AUC: 0.9852\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:58<00:00,  3.99it/s, Loss=0.0231]\n",
            "Validation: 100%|██████████| 409/409 [00:33<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.98      0.87      0.92      3114\n",
            "        Fake       0.96      0.99      0.98      9961\n",
            "\n",
            "    accuracy                           0.97     13075\n",
            "   macro avg       0.97      0.93      0.95     13075\n",
            "weighted avg       0.97      0.97      0.97     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0880 | Acc: 0.9686 | F1: 0.9797\n",
            "  Prec: 0.9660 | Rec: 0.9938 | AUC: 0.9910\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1015 | Acc: 0.9658 | F1: 0.9779\n",
            "  Prec: 0.9618 | Rec: 0.9946 | AUC: 0.9881\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:56<00:00,  4.01it/s, Loss=0.0353]\n",
            "Validation: 100%|██████████| 409/409 [00:31<00:00, 12.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.88      0.92      3114\n",
            "        Fake       0.96      0.99      0.98      9961\n",
            "\n",
            "    accuracy                           0.97     13075\n",
            "   macro avg       0.97      0.94      0.95     13075\n",
            "weighted avg       0.97      0.97      0.97     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0803 | Acc: 0.9715 | F1: 0.9816\n",
            "  Prec: 0.9689 | Rec: 0.9946 | AUC: 0.9924\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1081 | Acc: 0.9656 | F1: 0.9777\n",
            "  Prec: 0.9644 | Rec: 0.9915 | AUC: 0.9871\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [06:01<00:00,  3.96it/s, Loss=0.7272]\n",
            "Validation: 100%|██████████| 409/409 [00:31<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.88      0.93      3114\n",
            "        Fake       0.96      0.99      0.98      9961\n",
            "\n",
            "    accuracy                           0.97     13075\n",
            "   macro avg       0.97      0.94      0.95     13075\n",
            "weighted avg       0.97      0.97      0.97     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0759 | Acc: 0.9732 | F1: 0.9826\n",
            "  Prec: 0.9703 | Rec: 0.9953 | AUC: 0.9932\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1070 | Acc: 0.9662 | F1: 0.9781\n",
            "  Prec: 0.9647 | Rec: 0.9919 | AUC: 0.9876\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [06:00<00:00,  3.97it/s, Loss=0.8313]\n",
            "Validation: 100%|██████████| 409/409 [00:30<00:00, 13.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.89      0.93      3114\n",
            "        Fake       0.97      0.99      0.98      9961\n",
            "\n",
            "    accuracy                           0.97     13075\n",
            "   macro avg       0.97      0.94      0.95     13075\n",
            "weighted avg       0.97      0.97      0.97     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0706 | Acc: 0.9746 | F1: 0.9835\n",
            "  Prec: 0.9715 | Rec: 0.9959 | AUC: 0.9940\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1080 | Acc: 0.9671 | F1: 0.9787\n",
            "  Prec: 0.9652 | Rec: 0.9927 | AUC: 0.9876\n",
            "  --> Best model saved!\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [06:01<00:00,  3.96it/s, Loss=0.0003]\n",
            "Validation: 100%|██████████| 409/409 [00:31<00:00, 12.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.96      0.87      0.91      3114\n",
            "        Fake       0.96      0.99      0.97      9961\n",
            "\n",
            "    accuracy                           0.96     13075\n",
            "   macro avg       0.96      0.93      0.94     13075\n",
            "weighted avg       0.96      0.96      0.96     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.1031 | Acc: 0.9633 | F1: 0.9763\n",
            "  Prec: 0.9635 | Rec: 0.9894 | AUC: 0.9890\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1253 | Acc: 0.9599 | F1: 0.9741\n",
            "  Prec: 0.9607 | Rec: 0.9879 | AUC: 0.9849\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:58<00:00,  3.99it/s, Loss=0.1525]\n",
            "Validation: 100%|██████████| 409/409 [00:31<00:00, 12.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.94      0.88      0.91      3114\n",
            "        Fake       0.96      0.98      0.97      9961\n",
            "\n",
            "    accuracy                           0.96     13075\n",
            "   macro avg       0.95      0.93      0.94     13075\n",
            "weighted avg       0.96      0.96      0.96     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.1013 | Acc: 0.9639 | F1: 0.9766\n",
            "  Prec: 0.9636 | Rec: 0.9901 | AUC: 0.9892\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1356 | Acc: 0.9573 | F1: 0.9723\n",
            "  Prec: 0.9631 | Rec: 0.9816 | AUC: 0.9841\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [05:59<00:00,  3.98it/s, Loss=0.0386]\n",
            "Validation: 100%|██████████| 409/409 [00:30<00:00, 13.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.87      0.92      3114\n",
            "        Fake       0.96      0.99      0.98      9961\n",
            "\n",
            "    accuracy                           0.96     13075\n",
            "   macro avg       0.97      0.93      0.95     13075\n",
            "weighted avg       0.96      0.96      0.96     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0950 | Acc: 0.9671 | F1: 0.9787\n",
            "  Prec: 0.9662 | Rec: 0.9915 | AUC: 0.9903\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1137 | Acc: 0.9628 | F1: 0.9760\n",
            "  Prec: 0.9612 | Rec: 0.9913 | AUC: 0.9860\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1431/1431 [06:01<00:00,  3.96it/s, Loss=0.0051]\n",
            "Validation: 100%|██████████| 409/409 [00:31<00:00, 13.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.88      0.92      3114\n",
            "        Fake       0.96      0.99      0.98      9961\n",
            "\n",
            "    accuracy                           0.96     13075\n",
            "   macro avg       0.96      0.93      0.95     13075\n",
            "weighted avg       0.96      0.96      0.96     13075\n",
            "\n",
            "\n",
            "Train Metrics:\n",
            "  Loss: 0.0929 | Acc: 0.9676 | F1: 0.9790\n",
            "  Prec: 0.9672 | Rec: 0.9911 | AUC: 0.9909\n",
            "\n",
            "Val Metrics:\n",
            "  Loss: 0.1099 | Acc: 0.9636 | F1: 0.9764\n",
            "  Prec: 0.9629 | Rec: 0.9904 | AUC: 0.9870\n",
            "\n",
            "Early stopping triggered after 14 epochs\n",
            "FINAL TESTING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 205/205 [00:16<00:00, 12.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.98      0.88      0.93      1558\n",
            "        Fake       0.96      0.99      0.98      4982\n",
            "\n",
            "    accuracy                           0.97      6540\n",
            "   macro avg       0.97      0.94      0.95      6540\n",
            "weighted avg       0.97      0.97      0.97      6540\n",
            "\n",
            "\n",
            "==================== FINAL RESULTS ====================\n",
            "Test Accuracy:  0.9667\n",
            "Test F1 Score:  0.9784\n",
            "Test Precision: 0.9640\n",
            "Test Recall:    0.9934\n",
            "Test ROC AUC:   0.9866\n",
            "Test Loss:      0.1168\n",
            "====================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/working/confusion_matrix.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-662884489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted Label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/confusion_matrix.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/confusion_matrix.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_paths, train_labels = load_split_dataset(data_path, 'train')\n",
        "    val_paths, val_labels = load_split_dataset(data_path, 'val')\n",
        "    test_paths, test_labels = load_split_dataset(data_path, 'test')\n",
        "\n",
        "    print(f\"Train samples: {len(train_paths)}\")\n",
        "    print(f\"Val samples: {len(val_paths)}\")\n",
        "    print(f\"Test samples: {len(test_paths)}\")\n",
        "    print(f\"Train label distribution: {Counter(train_labels)}\")\n",
        "    print(f\"Val label distribution: {Counter(val_labels)}\")\n",
        "    print(f\"Test label distribution: {Counter(test_labels)}\")\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    model = EfficientNetClassifier('efficientnet_b0', num_classes=2)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    epochs = 15\n",
        "    best_val_f1 = 0\n",
        "    patience = 4\n",
        "    counter = 0\n",
        "\n",
        "    print(f\"\\nTraining: {model.__class__.__name__}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        train_loss, train_acc, train_f1, train_prec, train_rec, train_auc, _ = train_model_one_epoch(\n",
        "            model, train_loader, optimizer, loss_fn, device\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, val_f1, val_prec, val_rec, val_auc, _, _ = validate_model(\n",
        "            model, val_loader, loss_fn, device\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"\\nTrain Metrics:\")\n",
        "        print(f\"  Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
        "        print(f\"  Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | AUC: {train_auc:.4f}\")\n",
        "\n",
        "        print(f\"\\nVal Metrics:\")\n",
        "        print(f\"  Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
        "        print(f\"  Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | AUC: {val_auc:.4f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), \"/content/EfficientNet.pth\")\n",
        "            counter = 0\n",
        "            print(\"  --> Best model saved!\")\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"FINAL TESTING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"/content/EfficientNet.pth\"))\n",
        "\n",
        "    test_loss, test_acc, test_f1, test_prec, test_rec, test_auc, test_probs, test_true = validate_model(\n",
        "        model, test_loader, loss_fn, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*20} FINAL RESULTS {'='*20}\")\n",
        "    print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
        "    print(f\"Test F1 Score:  {test_f1:.4f}\")\n",
        "    print(f\"Test Precision: {test_prec:.4f}\")\n",
        "    print(f\"Test Recall:    {test_rec:.4f}\")\n",
        "    print(f\"Test ROC AUC:   {test_auc:.4f}\")\n",
        "    print(f\"Test Loss:      {test_loss:.4f}\")\n",
        "    print(\"=\"*52)\n",
        "\n",
        "\n",
        "    test_preds = (np.array(test_probs) > 0.5).astype(int)\n",
        "    cm = confusion_matrix(test_true, test_preds)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.title('Confusion Matrix - Test Set')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/kaggle/working/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z_hvHGiYJSS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6013040,
          "sourceId": 9809299,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python (local-pytorch-env)",
      "language": "python",
      "name": "local-pytorch-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "123e31bea58047de8a68c3938691534e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9237458ca5b7403f8d48cd22820a354b",
              "IPY_MODEL_5596af82c7b84ee58000329aa564ccd6",
              "IPY_MODEL_cbd039abe67b4e888be5500cf59ca4a4"
            ],
            "layout": "IPY_MODEL_8cfb2f16a79e4b96ad74e3ac1304ff2b"
          }
        },
        "9237458ca5b7403f8d48cd22820a354b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1859868b14e42c2ae9e04f9b3b7c63b",
            "placeholder": "​",
            "style": "IPY_MODEL_c9294f6e7150450d81ff5cb2c7066b27",
            "value": "model.safetensors: 100%"
          }
        },
        "5596af82c7b84ee58000329aa564ccd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f15a85266841b2a90818efa8d2f26f",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c94516a5b2794f82b96c3e101cf54714",
            "value": 21355344
          }
        },
        "cbd039abe67b4e888be5500cf59ca4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b0b14cf420648faa3386dad4a355d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_d474a6294f914674ab68f3677e1a9f68",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 196MB/s]"
          }
        },
        "8cfb2f16a79e4b96ad74e3ac1304ff2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1859868b14e42c2ae9e04f9b3b7c63b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9294f6e7150450d81ff5cb2c7066b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18f15a85266841b2a90818efa8d2f26f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c94516a5b2794f82b96c3e101cf54714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b0b14cf420648faa3386dad4a355d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d474a6294f914674ab68f3677e1a9f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}